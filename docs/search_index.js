var documenterSearchIndex = {"docs":
[{"location":"diffusion/#PDE-simulation,-machine-learning","page":"PDE simulation, machine learning","title":"PDE simulation, machine learning","text":"","category":"section"},{"location":"diffusion/","page":"PDE simulation, machine learning","title":"PDE simulation, machine learning","text":"DIR=joinpath(pwd(),\"../../src\") # change this to your EquivariantOperators.jl directory\r\n\"\"\"\r\nsimulation & machine learning of diffusion advection PDE\r\n\"\"\"\r\n\r\nusing Plots\r\nusing Random\r\nusing Flux\r\nusing LinearAlgebra\r\nRandom.seed!(1)\r\ninclude(\"$DIR/operators.jl\")\r\n\r\n# make grid\r\nn = 2\r\ndx = 0.02\r\ncell = dx * Matrix(I, n, n)\r\nrmax = 1.0\r\ngrid = Grid(cell, rmax)\r\nsz = size(grid)\r\n▽ = Op(:▽, cell)\r\n\r\n# put small blob as IC\r\nu0 = zeros(sz..., 1)\r\nput!(u0, grid, [0.0, 0.0], [1.0])\r\ng = Op(:Gaussian, cell; σ = dx)\r\nu0 = g(u0)\r\n\r\n# diffusion advection\r\np1 = [0.1]\r\np2 = [0.5, 0.4]\r\nvf = cat(p2[1] * ones(sz), p2[2] * ones(sz), dims = 3)\r\nf(u, p, t) = p1[1] * (▽ ⋅ ▽(u)) - vf ⊗ ▽(u)\r\n\r\n# simulate PDE\r\nusing DifferentialEquations\r\ntspan = (0.0, 1.0)\r\nprob = ODEProblem(f, u0, tspan)\r\nsol = solve(prob, Tsit5(), reltol = 1e-3, abstol = 1e-3)\r\n\r\n# plot\r\nusing Plots\r\ngr()\r\nanim = Animation()\r\n\r\nt = 0:0.02:1\r\nfor t in t\r\nheatmap(sol(t)[:, :, 1], clim=(0,10))\r\nframe(anim)\r\nend\r\ngif(anim, \"f.gif\", fps = 10)\r\n\r\n##\r\ndata = [(sol(t), f(sol(t), 0, 0)) for t in t]\r\n# op = Op(Radfunc(),-1e-6, 2dx, cell)\r\n# ps=Flux.params(op)\r\np1_ = ones(1)\r\np2_ = ones(2)\r\nps = Flux.params(p1_, p2_)\r\n\r\nfunction loss(u, du)\r\n    vf_ = cat(p2_[1] * ones(sz), p2_[2] * ones(sz), dims = 3)\r\n    duhat = p1_[1] .* (▽ ⋅ ▽(u)) - vf_ ⊗ ▽(u)\r\n    @show l = nae(duhat, du)\r\nend\r\n\r\nloss(data[1]...)\r\nopt = ADAM(0.1)\r\n\r\nFlux.@epochs 20 Flux.train!(loss, ps, data, opt)\r\n\r\n@show p1_, p2_\r\n# heatmap(op.kernel[:, :, 1])","category":"page"},{"location":"diffusion/","page":"PDE simulation, machine learning","title":"PDE simulation, machine learning","text":"(Image: )","category":"page"},{"location":"electrostatics/#Operators-basics,-machine-learning,-inverse-problems","page":"Operators basics, machine learning, inverse problems","title":"Operators basics, machine learning, inverse problems","text":"","category":"section"},{"location":"electrostatics/","page":"Operators basics, machine learning, inverse problems","title":"Operators basics, machine learning, inverse problems","text":"DIR = joinpath(pwd(), \"../../src\") # change this to your EquivariantOperators.jl directory\r\n\"\"\"\r\nFinite difference calculation & machine learning of electric potential & electric field from charge\r\n\"\"\"\r\n\r\nusing LinearAlgebra\r\nusing Plots\r\nusing Random\r\nusing Flux\r\nRandom.seed!(1)\r\ninclude(\"$DIR/operators.jl\")\r\n\r\n# make grid\r\ndims = 2\r\ndx = 0.1\r\ncell = dx * Matrix(I, dims, dims)\r\nrmax = 1.0\r\ngrid = Grid(cell, rmax)\r\n\r\n# make operators\r\nrmin = 1e-9\r\nrmax = sqrt(3)\r\nϕ = Op(r -> 1 / (4π * r), rmin, rmax, cell)\r\nE = Op(r -> 1 / (4π * r^2), rmin, rmax, cell; l = 1)\r\n▽ = Op(:▽, cell)\r\n\r\n# put dipole charges\r\nρf = zeros(size(grid)..., 1)\r\nput!(ρf, grid, [0.5 0.0; -0.5 0.0]', [1.0 -1]'')\r\n\r\n# calculate fields\r\nEf = E(ρf)\r\nϕf = ϕ(ρf)\r\n\r\n# test\r\nrvec = [0, 0]\r\n@show get(ϕf, grid, rvec), [0.0]\r\n@show get(Ef, grid, rvec), get(-▽(ϕ(ρf)), grid, rvec), [-2 / (4π * 0.5^2), 0]\r\n\r\np = []\r\npush!(p, heatmap(ρf[:, :, 1]',title = \"dipole charge\"), )\r\npush!(p, heatmap(ϕf[:, :, 1]', title = \"dipole potential\"),)\r\nplot(p..., layout = length(p))\r\nsavefig(\"d1.svg\");\r\n\r\n# @unpack x, y=grid\r\nx = grid.coords[:, :, 1]\r\ny = grid.coords[:, :, 2]\r\ns = 1e-2\r\nu = s * Ef[:, :, 1]\r\nv = s * Ef[:, :, 2]\r\nx, y, u, v = vec.([x, y, u, v])\r\nquiver(x, y, quiver = (u, v), c = :blue, title = \"dipole electric field\")\r\nsavefig(\"d2.svg\");\r\n\r\n##\r\n# make neural operators\r\nϕ_ = Op(Radfunc(), rmin, rmax, cell)\r\nE_ = Op(Radfunc(), rmin, rmax, cell; l = 1)\r\n\r\nps = Flux.params(ϕ_, E_)\r\nfunction loss()\r\n    remake!(E_)\r\n    remake!(ϕ_)\r\n    global E_f = E_(ρf)\r\n    global ϕ_f = ϕ_(ρf)\r\n    @show l = (nae(E_f, Ef) + nae(ϕ_f, ϕf)) / 2\r\n    l\r\nend\r\n\r\ndata = [()]\r\nloss()\r\nopt = ADAM(0.1)\r\nFlux.@epochs 50 Flux.train!(loss, ps, data, opt)\r\n\r\n## plot\r\np = []\r\npush!(p, heatmap(ϕ_.kernel[:, :, 1], title = \"learned potential kernel\"))\r\nr = 0:0.01:1\r\npush!(\r\n    p,\r\n    plot(r, ϕ_.radfunc.(r), title = \"learned potential kernel radial function\"),\r\n)\r\nplot(p..., layout = length(p))\r\nsavefig(\"ml1.svg\");\r\n\r\nx = E_.grid.coords[:, :, 1]\r\ny = E_.grid.coords[:, :, 2]\r\ns = 1e-2\r\nu = s * E_.kernel[:, :, 1]\r\nv = s * E_.kernel[:, :, 2]\r\nx, y, u, v = vec.([x, y, u, v])\r\nquiver(x, y, quiver = (u, v), c = :blue, title = \"learned E field kernel \")\r\nsavefig(\"ml2.svg\");\r\n","category":"page"},{"location":"electrostatics/","page":"Operators basics, machine learning, inverse problems","title":"Operators basics, machine learning, inverse problems","text":"(Image: ) (Image: ) (Image: ) (Image: )","category":"page"},{"location":"tutorials/#Tutorials","page":"Tutorials","title":"Tutorials","text":"","category":"section"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"We're still beta testing before pushing to Julia registry. Meanwhile, do","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"git clone https://github.com/aced-differentiate/EquivariantOperators.jl","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Run setup.jl to install dependencies. (Sorry *.toml file isn't in working order.)","category":"page"},{"location":"tutorials/","page":"Tutorials","title":"Tutorials","text":"Pages = [\"electrostatics.md\", \"diffusion.md\"]\r\nDepth = 3","category":"page"},{"location":"publications/#Publications","page":"Publications","title":"Publications","text":"","category":"section"},{"location":"publications/","page":"Publications","title":"Publications","text":"Preprint: Paul Shen, Michael Herbst, Venkat Viswanathan. Rotation Equivariant Fourier Neural Operators for Learning Symmetry Preserving Transformations on Scalar Fields, Vector Fields, and Higher Order Tensor Fields. Arxiv. 2021.","category":"page"},{"location":"architecture/#Architecture","page":"Architecture","title":"Architecture","text":"","category":"section"},{"location":"architecture/#Scalar-and-vector-fields","page":"Architecture","title":"Scalar & vector fields","text":"","category":"section"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"Scalar & vector fields in 2d/3d are represented as 3d/4d arrays with the last dimension for the field component. For example, a 3d vector field would be sized XxYxZx3 and while a 2d scalar field would be XxYx1.","category":"page"},{"location":"architecture/#Customizable-grid","page":"Architecture","title":"Customizable grid","text":"","category":"section"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"Grid is specified by its discrete cell vectors (column-wise matrix), overall size and origin. For a uniform Cartesian 5x5x5 grid discretized at 0.1 with a centered origin, we get cell = [0.1 0; 0 0.1] & origin = [3, 3, 3]. Grid cell can in general be nonuniform & noncartesian.","category":"page"},{"location":"architecture/#Pointwise-products","page":"Architecture","title":"Pointwise products","text":"","category":"section"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"u ⊗ v computes the appropriate pointwise product between 2 scalar or vector fields (inferred as scalar-scalar, scalar-vector, dot, cross). For greater clarity one may also write u ⋅ v for dot and u ⨉ v for cross","category":"page"},{"location":"architecture/#Particle-mesh-placement-and-interpolation","page":"Architecture","title":"Particle mesh placement and interpolation","text":"","category":"section"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"With grid info we can interpolate a scalar or vector field at any location. We can also place a scalar or vector point source anywhere with automatic normalization wrt discretization. Both work via a proximity weighted average of the closest grid points (in general up to 4 in 2d and 8 in 3d).","category":"page"},{"location":"architecture/#Finite-difference-operators","page":"Architecture","title":"Finite difference operators","text":"","category":"section"},{"location":"architecture/","page":"Architecture","title":"Architecture","text":"Op constructs finite difference operators. Prebuilt operators like differential operators (▽) & common Green's functions can be specified by name. Custom equivariant operators can be made by specifying radial function.","category":"page"},{"location":"#Home","page":"Home","title":"Home","text":"","category":"section"},{"location":"#Synopsis","page":"Home","title":"Synopsis","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"EquivariantOperators.jl implements in Julia fully differentiable finite difference operators on scalar or vector fields in 2d/3d. It can run forwards for PDE simulation or image processing, or back propagated for machine learning or inverse problems. Emphasis is on symmetry preserving rotation equivariant operators, including differential operators, common Green's functions & parametrized neural operators. Supports possibly nonuniform, nonorthogonal or periodic grids.","category":"page"},{"location":"#Theory","page":"Home","title":"Theory","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Equivariant linear operators are our building blocks. Equivariance means a rotation of the input results in the same rotation of the output thus preserving symmetry. Applying a linear operator convolves the input with the operator's kernel. If the operator is also equivariant, then its kernel must be radially symmetric. Differential operators and Green's functions are in fact equivariant linear operators. We provide built in constructors for these common operators. By parameterizing the radial function, we can also construct custom neural equivariant operators for machine learning.","category":"page"},{"location":"#Publications","page":"Home","title":"Publications","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Preprint: Paul Shen, Michael Herbst, Venkat Viswanathan. Rotation Equivariant Fourier Neural Operators for Learning Symmetry Preserving Transformations on Scalar Fields, Vector Fields, and Higher Order Tensor Fields. Arxiv. 2021.","category":"page"},{"location":"#Contributors","page":"Home","title":"Contributors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Paul Shen (xingpins@andrew.cmu.edu), Michael Herbst (herbst@acom.rwth-aachen.de), Venkat Viswanathan (venkatv@andrew.cmu.edu)","category":"page"},{"location":"","page":"Home","title":"Home","text":"In consultation with Rachel Kurchin, Dhairya Gandhi, Chris Rackauckas","category":"page"},{"location":"","page":"Home","title":"Home","text":"In collaboration with Julia Computing","category":"page"}]
}
